import json
from pathlib import Path

nb_path = Path("04_Notebooks/nonparametric_analysis_template.ipynb")


def code(source):
    return {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": (
            source
            if isinstance(source, list)
            else [l + "\n" for l in source.split("\n")]
        ),
    }


def md(source):
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": (
            source
            if isinstance(source, list)
            else [l + "\n" for l in source.split("\n")]
        ),
    }


cells = []

# Header
cells.append(
    md(
        [
            "# ë¹„ëª¨ìˆ˜ ë¶„ì„ ì¢…í•© í…œí”Œë¦¿ (All-in-One)",
            "",
            "ì´ ë…¸íŠ¸ë¶ì€ `nonparametric_analysis` íŒ¨í‚¤ì§€ê°€ ì œê³µí•˜ëŠ” **17ì¢…ì˜ ëª¨ë“  ë¶„ì„ ê¸°ëŠ¥**ì„ ì˜ˆì‹œì™€ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.",
            "ê° ê²°ê³¼ ì•„ë˜ì—ëŠ” **í†µê³„ ë¹„ì „ë¬¸ê°€ë¥¼ ìœ„í•œ 'ê²°ê³¼ í•´ì„ ê°€ì´ë“œ'**ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        ]
    )
)

# Setup
cells.append(
    code(
        [
            "import sys",
            "from pathlib import Path",
            "import pandas as pd",
            "import numpy as np",
            "",
            "%load_ext autoreload",
            "%autoreload 2",
            "",
            "# Add src to path",
            "sys.path.append(str(Path('../03_Code/src').resolve()))",
            "",
            "from nonparametric_analysis.analysis import nonparametric_methods as np_methods",
            "from nonparametric_analysis.analysis import utils",
            "from nonparametric_analysis.analysis.visualizations import setup_visualization",
            "",
            "setup_visualization() # í•œê¸€ í°íŠ¸ ë° ìŠ¤íƒ€ì¼ ì„¤ì •",
        ]
    )
)

# Load Data
cells.append(
    code(
        [
            "# ë°ì´í„° ë¡œë“œ",
            "data_path = Path('../02_Data/sample_nonparametric.csv')",
            "df = pd.read_csv(data_path)",
            "series = df['feature_1'].dropna()",
            "",
            "# ê·¸ë£¹ ë¶„í•  (ì˜ˆì‹œ: feature_2 ê¸°ì¤€ 50 ì´ˆê³¼/ì´í•˜)",
            "group_a = df[df['feature_2'] > 50]['feature_1']",
            "group_b = df[df['feature_2'] <= 50]['feature_1']",
            "",
            "# ì§ì§€ì–´ì§„ ë°ì´í„° ì˜ˆì‹œ (Paired)",
            "before = series[:30].values",
            "after = before + np.random.normal(0.5, 1, 30) # ì•½ê°„ì˜ ë³€í™” ì¶”ê°€",
            "",
            "# 3ê°œ ê·¸ë£¹ ë°ì´í„° ì˜ˆì‹œ (Repeated / Multi)",
            "t1 = series[:30].values",
            "t2 = series[30:60].values",
            "t3 = series[60:90].values",
            "",
            "print(f'Data Loaded: Series N={len(series)}, Group A={len(group_a)}, Group B={len(group_b)}')",
        ]
    )
)

# --- Part 1: ë‹¨ì¼ ë³€ìˆ˜ ë¶„ì„ ---
cells.append(md("## 1. ë‹¨ì¼ ë³€ìˆ˜ ë¶„ì„ (Single Variable)"))

# 1.1 Normality
cells.append(md("### 1.1 ì •ê·œì„± ê²€ì • (Normality Test)"))
cells.append(code('res = np_methods.test_normality(series, name="Feature 1")'))
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- **Is Normal: True** â†’ ë°ì´í„°ê°€ ì¢… ëª¨ì–‘(ì •ê·œë¶„í¬)ì…ë‹ˆë‹¤.",
            "- **Is Normal: False** â†’ ë°ì´í„°ê°€ ì¹˜ìš°ì³ ìˆìŠµë‹ˆë‹¤. (ë¹„ëª¨ìˆ˜ ë¶„ì„ ê¶Œì¥)",
        ]
    )
)

# 1.2 Runs Test
cells.append(md("### 1.2 ëŸ° ê²€ì • (Runs Test - ë¬´ì‘ìœ„ì„±)"))
cells.append(code('res = np_methods.runs_test_analysis(series, name="Feature 1")'))
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ë°ì´í„°ê°€ **ë¬´ì‘ìœ„(Random)**ë¡œ ë¶„í¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.",
            "- **p < 0.05**: ë¬´ì‘ìœ„ê°€ ì•„ë‹™ë‹ˆë‹¤. (ì–´ë–¤ íŒ¨í„´ì´ ì¡´ì¬í•¨)",
        ]
    )
)

# 1.3 Mann-Kendall
cells.append(md("### 1.3 ì¶”ì„¸ ë¶„ì„ (Mann-Kendall Trend)"))
cells.append(code('res = np_methods.mann_kendall_test(series, name="Feature 1")'))
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- **Trend**: increasing(ì¦ê°€), decreasing(ê°ì†Œ), no trend(ê²½í–¥ ì—†ìŒ)",
            "- **Slope**: ë³€í™” ì†ë„ (ì–‘ìˆ˜ë©´ ì¦ê°€, ìŒìˆ˜ë©´ ê°ì†Œ)",
        ]
    )
)

# 1.4 Pettitt
cells.append(md("### 1.4 ë³€ê³¡ì  íƒì§€ (Pettitt Test)"))
cells.append(
    code(
        [
            'res = np_methods.pettitt_test(series, name="Feature 1")',
            "if res['change_point']:",
            "    print(f\"Change Point Index: {res['change_point']}\")",
        ]
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ë°ì´í„°ì˜ íë¦„(í‰ê· )ì´ **ê°‘ìê¸° ë°”ë€ŒëŠ” ì§€ì **ì„ ì°¾ìŠµë‹ˆë‹¤.",
        ]
    )
)

# 1.5 PELT
cells.append(md("### 1.5 ë‹¤ì¤‘ êµ¬ê°„ ë¶„í•  (PELT)"))
cells.append(
    code('res = np_methods.detect_changepoints_pelt(series, name="Feature 1")')
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ë°ì´í„°ì˜ íŒ¨í„´ì´ ë°”ë€ŒëŠ” **ì—¬ëŸ¬ ì§€ì **ì„ ë™ì‹œì— ì°¾ìŠµë‹ˆë‹¤.",
        ]
    )
)

# --- Part 2: ê·¸ë£¹ ë¹„êµ ---
cells.append(md("## 2. ê·¸ë£¹ ë¹„êµ (Group Comparison)"))

# 2.1 Mann-Whitney
cells.append(md("### 2.1 ë‘ ë…ë¦½ ê·¸ë£¹ ë¹„êµ (Mann-Whitney U)"))
cells.append(
    code(
        'res = np_methods.mann_whitney_test(group_a, group_b, name_a="Group A", name_b="Group B")'
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ì„œë¡œ ë‹¤ë¥¸ ë‘ ê·¸ë£¹(A vs B)ì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.",
            "- **p < 0.05**: ë‘ ê·¸ë£¹ì€ í†µê³„ì ìœ¼ë¡œ **ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.**",
        ]
    )
)

# 2.2 Kolmogorov-Smirnov
cells.append(md("### 2.2 ë‘ ë¶„í¬ ë¹„êµ (K-S Test)"))
cells.append(
    code(
        'res = np_methods.ks_test(group_a, group_b, name_a="Group A", name_b="Group B")'
    )
)
cells.append(
    md(["#### ğŸ’¡ ê²°ê³¼ í•´ì„", "- ë‘ ë°ì´í„°ì˜ **ë¶„í¬ ëª¨ì–‘** ìì²´ê°€ ë‹¤ë¥¸ì§€ ë´…ë‹ˆë‹¤."])
)

# 2.3 Wilcoxon Paired
cells.append(md("### 2.3 ì§ì§€ì–´ì§„ ê·¸ë£¹ ë¹„êµ (Wilcoxon Signed Rank)"))
cells.append(
    code(
        'res = np_methods.wilcoxon_paired_test(before, after, name_a="Before", name_b="After")'
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- **ì „/í›„(Before/After)**ì™€ ê°™ì´ ì§ì„ ì´ë£¬ ë°ì´í„°ì˜ ë³€í™”ë¥¼ ë´…ë‹ˆë‹¤.",
            "- **p < 0.05**: ì „í›„ì— ìœ ì˜ë¯¸í•œ ë³€í™”ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.",
        ]
    )
)

# 2.4 Sign Test
cells.append(md("### 2.4 ë¶€í˜¸ ê²€ì • (Sign Test)"))
cells.append(code("res = np_methods.sign_test(before, after)"))
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ë³€í™”ì˜ í¬ê¸°ë³´ë‹¤ëŠ” **ì¦ê°€í–ˆë‚˜/ê°ì†Œí–ˆë‚˜(ë°©í–¥)**ë§Œ ë´…ë‹ˆë‹¤.",
        ]
    )
)

# 2.5 Kruskal-Wallis
cells.append(md("### 2.5 ì„¸ ë…ë¦½ ê·¸ë£¹ ë¹„êµ (Kruskal-Wallis)"))
cells.append(
    code(
        'res = np_methods.kruskal_wallis_test([t1, t2, t3], group_names=["G1", "G2", "G3"])'
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ì„œë¡œ ë‹¤ë¥¸ 3ê°œ ì´ìƒì˜ ê·¸ë£¹ ì¤‘ **ì ì–´ë„ í•˜ë‚˜ëŠ” ë‹¤ë¥¸ì§€** í™•ì¸í•©ë‹ˆë‹¤.",
            "- **p < 0.05**: ê·¸ë£¹ ê°„ì— ì°¨ì´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.",
        ]
    )
)

# 2.6 Friedman
cells.append(md("### 2.6 ë°˜ë³µ ì¸¡ì • ë¹„êµ (Friedman Test)"))
cells.append(
    code(
        [
            "# ë°ì´í„° ê¸¸ì´ ë§ì¶¤ (í•„ìˆ˜)",
            "data_matrix = [t1, t2, t3]",
            'res = np_methods.friedman_test(data_matrix, group_names=["Time1", "Time2", "Time3"])',
        ]
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- 3ê°œ ì´ìƒì˜ ì‹œì /ì¡°ê±´ì—ì„œ **ë™ì¼í•œ ëŒ€ìƒ**ì„ ë°˜ë³µ ì¸¡ì •í–ˆì„ ë•Œ ì°¨ì´ë¥¼ ë´…ë‹ˆë‹¤.",
        ]
    )
)

# --- Part 3: ìƒê´€ ë¶„ì„ ---
cells.append(md("## 3. ìƒê´€ ê´€ê³„ (Correlation)"))

# 3.1 Spearman Matrix
cells.append(md("### 3.1 ìƒê´€ í–‰ë ¬ (Heatmap)"))
cells.append(code("res = np_methods.correlation_matrix_nonparametric(df)"))
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ì—¬ëŸ¬ ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ í•œëˆˆì— ë´…ë‹ˆë‹¤.",
            "- **ë¶‰ì€ìƒ‰**: ê´€ê³„ ì—†ìŒ / **ì´ˆë¡ìƒ‰**: ê´€ê³„ ìˆìŒ",
        ]
    )
)

# 3.2 Kendall / Distance
cells.append(md("### 3.2 ë‹¤ì–‘í•œ ìƒê´€ ë¶„ì„ (Kendall, Distance)"))
cells.append(
    code(
        [
            "x = df['feature_1'][:50]",
            "y = df['feature_2'][:50]",
            "",
            "res_k = np_methods.kendall_corr(x, y)",
            "print(f\"Kendall Tau: {res_k['correlation']:.4f}\")",
            "",
            "res_d = np_methods.distance_correlation(x, y)",
            "print(f\"Distance Corr: {res_d['correlation']:.4f}\")",
        ]
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- **Kendall**: ìˆœìœ„ ë™ì ì´ ë§ì„ ë•Œ ë” ì •í™•í•©ë‹ˆë‹¤.",
            "- **Distance**: ê³¡ì„  ê´€ê³„(ë¹„ì„ í˜•)ë„ ì°¾ì•„ëƒ…ë‹ˆë‹¤. (0=ë…ë¦½, 1=ì¢…ì†)",
        ]
    )
)

# --- Part 4: ë¦¬ìƒ˜í”Œë§ ---
cells.append(md("## 4. ë¦¬ìƒ˜í”Œë§ ê¸°ë²• (Resampling)"))

# 4.1 Bootstrap
cells.append(md("### 4.1 ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ (Bootstrap CI)"))
cells.append(
    code(
        [
            "res = np_methods.bootstrap_ci(series, stat_func=np.mean, n_boot=1000)",
            "print(f\"Bootstrap 95% CI: {res['ci_lower']:.4f} ~ {res['ci_upper']:.4f}\")",
        ]
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ë°ì´í„°ê°€ ì ì„ ë•Œ, í†µê³„ëŸ‰(í‰ê·  ë“±)ì˜ **ì‹ ë¢°êµ¬ê°„**ì„ ì¶”ì •í•©ë‹ˆë‹¤.",
        ]
    )
)

# 4.2 Permutation
cells.append(md("### 4.2 ìˆœì—´ ê²€ì • (Permutation Test)"))
cells.append(
    code(
        [
            "stat_func = lambda x, y: np.mean(x) - np.mean(y)",
            "res = np_methods.permutation_test(group_a, group_b, stat_func=stat_func, n_perm=1000)",
            "print(f\"Permutation p-value: {res['p_value']:.4f}\")",
        ]
    )
)
cells.append(
    md(
        [
            "#### ğŸ’¡ ê²°ê³¼ í•´ì„",
            "- ì •ê·œë¶„í¬ ê°€ì •ì´ ë¶ˆê°€ëŠ¥í•  ë•Œ, **ë‘ ê·¸ë£¹ì˜ ì°¨ì´ê°€ ìš°ì—°ì¸ì§€** ê²€ì •í•©ë‹ˆë‹¤.",
        ]
    )
)

# Save
nb_content = {"cells": cells, "metadata": {}, "nbformat": 4, "nbformat_minor": 5}
with open(nb_path, "w", encoding="utf-8") as f:
    json.dump(nb_content, f, indent=4)
