# 비모수 데이터 통계 분석 이론 가이드

> **목적:** 분석 결과 해석, 보고서 작성 시 참조하는 이론·해석·코드 레퍼런스  
> **관련 문서:** 사양서 `01_비모수_분석_spec.md` | 보고서 형식 `03_비모수_보고서_템플릿.md`

---

## 1. 비모수 통계 총론

### 1.1 비모수 검정이란?

**모수 검정**은 데이터가 특정 분포(대부분 정규분포)를 따른다고 가정하고, 그 분포의 모수(평균, 분산)를 추정·검정한다. **비모수 검정**은 분포 가정을 최소화하여 분석하는 방법이다.

"비모수"는 "모수가 없다"가 아니라 **"분포 자유(distribution-free)"**, 즉 데이터의 분포 형태를 특정하지 않는다는 의미이다.

### 1.2 핵심 원리: 순위(Rank)

대부분의 비모수 검정은 원래 값 대신 **순위**를 사용한다.

```
원본:  [3.2, 1.5, 8.7, 2.1, 5.0]
순위:  [3,   1,   5,   2,   4  ]
```

**순위 기반 접근의 장점:**
- 이상치에 강건: 8.7이 800.7이 되어도 순위 5는 동일
- 분포 형태에 무관: 순위는 항상 균등분포
- 순서형 데이터에 직접 적용 가능

### 1.3 모수 vs 비모수 비교

| 항목 | 모수 검정 | 비모수 검정 |
|------|----------|------------|
| 분포 가정 | 정규분포 등 명시적 가정 | 분포 가정 없음 |
| 대표값 | 평균(mean) | 중앙값(median) |
| 산포도 | 표준편차(SD) | IQR, MAD |
| 검정력 | 가정 충족 시 100% 기준 | ~95.5% (ARE) |
| 이상치 | 민감 | 강건 |
| 데이터 유형 | 연속형(등간/비율) | 순서형 이상 모두 |

### 1.4 검정력(Power) 이슈

비모수 검정의 **점근 상대 효율(ARE)**은 약 0.955(Wilcoxon vs t-검정). 가정이 충족되면 모수 검정이 약간 강하지만, 가정이 위반되면 비모수 검정이 오히려 더 높은 검정력을 보인다.

> **핵심:** 비모수 검정은 "차선책"이 아니라, 데이터 특성에 맞는 **"적합한 도구"**이다.

### 1.5 3종 보고 원칙

p-value 단독 해석은 금지한다. 다음 3가지를 항상 함께 보고:

1. **유의성:** p-value
2. **효과 크기:** r, ρ, τ, η², Cliff's δ 등
3. **불확실성:** Bootstrap 신뢰구간 또는 SE

### 1.6 오해 방지

1. "비모수라서 아무 가정도 없다" → **틀림.** 독립성, 대칭성 등 방법별 가정 존재
2. "p < 0.05면 중요하다" → **불완전.** 효과 크기 없이는 실질적 중요도 판단 불가
3. "상관이 있다 = 인과가 있다" → **금지.** 상관은 인과를 의미하지 않음
4. "변곡점 1개 탐지 = 확정" → **위험.** 알고리즘과 파라미터에 따라 결과 변동

---

## 2. 공통 유틸리티 함수

```python
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import (
    shapiro, mannwhitneyu, wilcoxon, kruskal,
    friedmanchisquare, spearmanr, kendalltau, ks_2samp
)
import scikit_posthocs as sp
import ruptures as rpt
import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
sns.set_theme(style="whitegrid", font="AppleGothic")

def interpret_p_value(p_value, alpha=0.05):
    """p-value 해석"""
    if p_value < 0.001:
        return f"p = {p_value:.4f} → 매우 강한 유의성 (p < 0.001)"
    elif p_value < alpha:
        return f"p = {p_value:.4f} → 유의함 (p < {alpha})"
    else:
        return f"p = {p_value:.4f} → 유의하지 않음 (p ≥ {alpha})"

def effect_size_r(z_stat, n):
    """효과 크기 r = |Z|/√N"""
    r = abs(z_stat) / np.sqrt(n)
    label = "큰" if r >= 0.5 else "중간" if r >= 0.3 else "작은"
    return r, label

def robust_descriptive(data, name="Feature"):
    """비모수 기술 통계 요약"""
    q1, med, q3 = np.percentile(data, [25, 50, 75])
    return pd.DataFrame({
        '지표': ['N','중앙값','Q1','Q3','IQR','최솟값','최댓값','MAD','왜도','첨도'],
        name: [len(data), f'{med:.3f}', f'{q1:.3f}', f'{q3:.3f}',
               f'{q3-q1:.3f}', f'{np.min(data):.3f}', f'{np.max(data):.3f}',
               f'{np.median(np.abs(data-med)):.3f}',
               f'{stats.skew(data):.3f}', f'{stats.kurtosis(data):.3f}']
    })
```

---

## 3. 단일 Feature 분석

### 3.1 정규성 검정 (Shapiro-Wilk)

**이론:** 순서통계량과 정규분포 기대값 간 상관계수를 계산. W ∈ [0,1], 1에 가까울수록 정규.

- **수식:** W = (Σ aᵢ x₍ᵢ₎)² / Σ(xᵢ - x̄)²
- **소표본(n < 50)에서 가장 강력**, 대표본에서는 사소한 이탈도 유의 → Q-Q 플롯 병행
- **대안:** Anderson-Darling (꼬리 민감), D'Agostino-Pearson (n ≥ 20)

```python
def test_normality(data, name="Feature", alpha=0.05):
    stat, p_value = shapiro(data)
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    axes[0].hist(data, bins='auto', density=True, alpha=0.7, color='steelblue', edgecolor='white')
    kde = stats.gaussian_kde(data)
    x = np.linspace(min(data), max(data), 100)
    axes[0].plot(x, kde(x), 'r-', lw=2)
    axes[0].plot(x, stats.norm.pdf(x, np.mean(data), np.std(data)), 'g--', lw=1.5)
    axes[0].set_title(f'{name} 분포')
    stats.probplot(data, plot=axes[1])
    axes[1].set_title('Q-Q Plot')
    axes[2].boxplot(data, vert=True, patch_artist=True, boxprops=dict(facecolor='lightyellow'))
    axes[2].set_title('Box Plot')
    plt.suptitle(f'Shapiro-Wilk: W={stat:.4f}, p={p_value:.4f}', y=1.02)
    plt.tight_layout(); plt.show()
    return {'statistic': stat, 'p_value': p_value, 'is_normal': p_value >= alpha}
```

**해석:** W ≈ 1.0이고 p ≥ 0.05 → 정규 가정 유지. W < 0.90이고 p < 0.01 → 비모수 필수.

**보고서 문장:** "Shapiro-Wilk 검정 결과, [변수]는 정규분포를 따르지 않았다(W = [값], p = [값]). 이후 비모수 검정을 적용하였다."

---

### 3.2 부호 검정 (Sign Test)

**이론:** 가장 단순한 비모수 검정. 중앙값이 θ₀이면 관측값이 θ₀보다 큰 확률 = 0.5이므로, 양수/음수 개수가 이항분포 B(n, 0.5)를 따르는지 검정.

- **가정:** 독립성만 필요. 크기 정보 무시 → 검정력 낮음
- **사용:** 극소 표본(n < 10), 이상치 완전 배제 필요시

```python
def sign_test(data, hypothesized_median, name="Feature"):
    diff = np.array(data) - hypothesized_median
    diff = diff[diff != 0]
    n_pos, n_neg = np.sum(diff > 0), np.sum(diff < 0)
    n = n_pos + n_neg
    p_value = min(2 * stats.binom.cdf(min(n_pos, n_neg), n, 0.5), 1.0)
    fig, ax = plt.subplots(figsize=(8, 4))
    colors = ['#e74c3c' if d < 0 else '#2ecc71' for d in diff]
    ax.bar(range(len(diff)), diff, color=colors, edgecolor='white')
    ax.axhline(0, color='black', lw=1)
    ax.set_title(f'{name}: 부호 검정 (양수={n_pos}, 음수={n_neg}, p={p_value:.4f})')
    plt.show()
    return {'n_pos': n_pos, 'n_neg': n_neg, 'p_value': p_value}
```

**보고서 문장:** "부호 검정 결과, [변수]의 중앙값이 [θ₀]과 유의하게 달랐다(양수 [N]개, 음수 [N]개, p = [값])."

---

### 3.3 Wilcoxon 부호순위 (단일 표본)

**이론:** 부호 검정 + 순위 정보. 편차 |dᵢ|에 순위를 부여하고, 양수/음수 편차의 순위 합을 비교. ARE ≈ 0.955.

- **가정:** 편차 분포가 대칭적 (비대칭이면 → 부호 검정)
- **효과 크기:** r = |Z|/√N

```python
def wilcoxon_one_sample(data, hypothesized_median, name="Feature"):
    diff = np.array(data) - hypothesized_median
    stat, p_value = wilcoxon(diff, alternative='two-sided')
    n = len(diff[diff != 0])
    z = (stat - n*(n+1)/4) / np.sqrt(n*(n+1)*(2*n+1)/24)
    r, r_label = effect_size_r(z, n)
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.boxplot(data, vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))
    ax.axvline(hypothesized_median, color='red', ls='--', label=f'H₀={hypothesized_median}')
    ax.axvline(np.median(data), color='green', ls='-', label=f'Mdn={np.median(data):.2f}')
    ax.legend()
    ax.set_title(f'{name}: Wilcoxon (T={stat:.1f}, p={p_value:.4f}, r={r:.3f})')
    plt.show()
    return {'statistic': stat, 'p_value': p_value, 'effect_size_r': r}
```

**보고서 문장:** "Wilcoxon 부호순위 검정 결과, [변수]의 중앙값(Mdn=[값])은 [θ₀]과 유의한 차이를 보였다(T=[값], Z=[값], p=[값], r=[값])."

---

### 3.4 Mann-Kendall + Sen's slope (추세 검정)

**이론:** 시계열의 **단조 추세** 유무를 검정. 모든 관측값 쌍(xᵢ, xⱼ)에서 증가/감소/동률을 카운트하여 Kendall τ를 계산. Sen's slope는 모든 쌍의 기울기 중앙값으로 추세 크기를 추정.

- **가정:** 관측치 독립성. 자기상관이 강하면 p-value 과소추정 주의
- **효과 크기:** τ (방향과 강도), Sen's slope (변화율)

```python
import pymannkendall as mk  # uv add pymannkendall

def mann_kendall_test(data, name="Feature"):
    result = mk.original_test(data)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(data, 'b-o', markersize=4, alpha=0.7)
    x_line = np.arange(len(data))
    ax.plot(x_line, result.intercept + result.slope * x_line, 'r-', lw=2,
            label=f"Sen's slope={result.slope:.3f}")
    ax.set_title(f"{name}: Mann-Kendall (τ={result.Tau:.3f}, p={result.p:.4f}, {result.trend})")
    ax.legend()
    plt.show()
    return {'tau': result.Tau, 'p_value': result.p, 'slope': result.slope, 'trend': result.trend}
```

**보고서 문장:** "[변수]는 기간 [t0~t1]에서 유의한 [증가/감소] 추세를 보였다(τ=[값], p=[값], Sen's slope=[값])."

---

### 3.5 Pettitt test (단일 변곡점)

**이론:** 시계열에서 **분포가 변화하는 단일 시점**을 탐지. Mann-Whitney U 통계량을 모든 분할 시점에 대해 계산하고, 최대값의 위치를 변곡점으로 추정.

- **가정:** 변곡점이 최대 1개 (다중 변곡점 → PELT 사용)
- **효과 크기:** 변화 전후 중앙값 차이

```python
import pymannkendall as mk

def pettitt_test(data, name="Feature"):
    n = len(data)
    U = np.zeros(n)
    for t in range(n):
        for i in range(t):
            for j in range(t, n):
                U[t] += np.sign(data[j] - data[i])
    K = np.max(np.abs(U))
    cp = np.argmax(np.abs(U))
    p_value = 2 * np.exp(-6 * K**2 / (n**3 + n**2))
    med_before = np.median(data[:cp])
    med_after = np.median(data[cp:])
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(data, 'b-o', markersize=4, alpha=0.7)
    ax.axvline(cp, color='red', ls='--', lw=2, label=f'변곡점 (idx={cp})')
    ax.hlines(med_before, 0, cp, colors='green', lw=2, label=f'전 Mdn={med_before:.2f}')
    ax.hlines(med_after, cp, n, colors='orange', lw=2, label=f'후 Mdn={med_after:.2f}')
    ax.set_title(f'{name}: Pettitt (K={K:.0f}, p={p_value:.4f})')
    ax.legend()
    plt.show()
    return {'change_point': cp, 'statistic': K, 'p_value': p_value,
            'median_before': med_before, 'median_after': med_after}
```

**보고서 문장:** "Pettitt 검정 결과 변화점은 t=[cp]에서 탐지되었고(p=[값]), 전후 중앙값은 [값]에서 [값]으로 변했다."

---

## 4. 두 Feature 비교 분석

### 4.1 Mann-Whitney U (독립 2그룹)

**이론:** 두 독립 그룹의 모든 관측값을 합쳐 순위를 매긴 후 각 그룹의 순위 합 비교. 독립표본 t-검정의 비모수 대안.

- **수식:** U = n₁n₂ + n₁(n₁+1)/2 - R₁
- **해석:** U는 "그룹1의 값이 그룹2보다 큰 쌍의 수" = 확률적 우위 측정
- **주의:** "중앙값 비교"가 **아님**. 두 분포 형태가 다르면 중앙값이 같아도 유의할 수 있음
- **효과 크기:** r, CLES(=U/(n₁×n₂)), Cliff's δ

```python
def mann_whitney_test(group1, group2, name1="G1", name2="G2"):
    stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')
    n1, n2, n = len(group1), len(group2), len(group1)+len(group2)
    mu_U = n1*n2/2
    sigma_U = np.sqrt(n1*n2*(n1+n2+1)/12)
    z = (stat - mu_U) / sigma_U
    r, r_label = effect_size_r(z, n)
    cles = stat / (n1*n2)
    cliffs_d = 2*cles - 1
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    bp = axes[0].boxplot([group1, group2], labels=[name1, name2], patch_artist=True, widths=0.5)
    bp['boxes'][0].set_facecolor('lightcoral'); bp['boxes'][1].set_facecolor('lightskyblue')
    for i, (g, c) in enumerate([(group1,'darkred'),(group2,'darkblue')]):
        axes[0].scatter(np.ones(len(g))*(i+1)+np.random.normal(0,.04,len(g)), g, alpha=.5, s=20, color=c)
    axes[0].set_title(f'U={stat:.1f}, p={p_value:.4f}')
    
    all_data = np.concatenate([group1, group2])
    ranks = stats.rankdata(all_data)
    axes[1].hist(ranks[:n1], bins='auto', alpha=.6, label=name1, color='coral')
    axes[1].hist(ranks[n1:], bins='auto', alpha=.6, label=name2, color='skyblue')
    axes[1].set_title('순위 분포'); axes[1].legend()
    plt.tight_layout(); plt.show()
    return {'statistic':stat, 'z':z, 'p_value':p_value, 'r':r, 'cles':cles, 'cliffs_delta':cliffs_d}
```

**해석 기준:** r: 0.1(작음)/0.3(중간)/0.5(큼). CLES: 0.5이면 차이 없음, 1.0이면 완전 우위.

**보고서 문장:** "Mann-Whitney U 결과, [G1](Mdn=[값])과 [G2](Mdn=[값]) 간 유의한 차이가 나타났다(U=[값], Z=[값], p=[값], r=[값]). CLES=[값]."

---

### 4.2 Wilcoxon signed-rank (대응)

**이론:** 쌍별 차이(dᵢ)를 구하고, |dᵢ|의 순위를 부여. 양수/음수 차이의 순위 합 비교. 대응표본 t-검정의 비모수 대안.

- **가정:** 차이값 분포가 대칭적

```python
def wilcoxon_paired_test(before, after, name="측정"):
    diff = np.array(after) - np.array(before)
    stat, p_value = wilcoxon(before, after, alternative='two-sided')
    n = len(diff[diff != 0])
    z = (stat - n*(n+1)/4) / np.sqrt(n*(n+1)*(2*n+1)/24)
    r, r_label = effect_size_r(z, n)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    for i in range(len(before)):
        c = '#2ecc71' if after[i]>before[i] else '#e74c3c'
        axes[0].plot([0,1], [before[i],after[i]], 'o-', color=c, alpha=.6)
    axes[0].set_xticks([0,1]); axes[0].set_xticklabels(['사전','사후'])
    axes[0].set_title(f'{name}: 사전-사후 변화')
    axes[1].hist(diff, bins='auto', color='salmon', edgecolor='white', alpha=.8)
    axes[1].axvline(0, color='black', ls='--')
    axes[1].axvline(np.median(diff), color='red', lw=2, label=f'Mdn diff={np.median(diff):.2f}')
    axes[1].set_title(f'T={stat:.1f}, p={p_value:.4f}'); axes[1].legend()
    plt.tight_layout(); plt.show()
    return {'statistic':stat, 'z':z, 'p_value':p_value, 'r':r, 'median_diff':np.median(diff)}
```

**보고서 문장:** "Wilcoxon 결과, 사전(Mdn=[값])과 사후(Mdn=[값]) 간 유의한 차이가 나타났다(T=[값], Z=[값], p=[값], r=[값])."

---

### 4.3 Kolmogorov-Smirnov (분포 형태 비교)

**이론:** 두 ECDF 간 최대 수직 거리 D를 검정. 위치뿐 아니라 산포·왜도 등 **분포의 모든 특성** 차이를 탐지. 중심부 차이에 더 민감.

```python
def ks_test(group1, group2, name1="G1", name2="G2"):
    stat, p_value = ks_2samp(group1, group2)
    fig, ax = plt.subplots(figsize=(10, 5))
    for d, n, c in [(group1,name1,'#3498db'),(group2,name2,'#e74c3c')]:
        s = np.sort(d); ecdf = np.arange(1,len(s)+1)/len(s)
        ax.step(s, ecdf, where='post', label=n, color=c, lw=2)
    ax.set_title(f'ECDF — KS D={stat:.3f}, p={p_value:.4f}')
    ax.legend(); ax.grid(True, alpha=.3); plt.show()
    return {'statistic': stat, 'p_value': p_value}
```

---

## 5. 다중 Feature 비교 분석

### 5.1 Kruskal-Wallis + Dunn 사후 검정

**이론:** 3개+ 독립 그룹의 분포 비교. 모든 관측값을 합쳐 순위를 매기고, 그룹별 순위 합이 기대와 다른지 검정. 일원분산분석(ANOVA)의 비모수 대안.

- **수식:** H = (12/N(N+1)) × Σ(Rᵢ²/nᵢ) - 3(N+1), df = k-1
- **효과 크기:** η² = (H - k + 1) / (N - k)
- **사후 검정:** Dunn's Test (Bonferroni or FDR 보정)

```python
def kruskal_wallis_test(*groups, group_names=None):
    stat, p_value = kruskal(*groups)
    k, N = len(groups), sum(len(g) for g in groups)
    if group_names is None: group_names = [f'G{i+1}' for i in range(k)]
    eta_sq = (stat - k + 1) / (N - k)
    eta_label = "큰" if eta_sq >= 0.14 else "중간" if eta_sq >= 0.06 else "작은"
    
    fig, ax = plt.subplots(figsize=(10, 5))
    bp = ax.boxplot(groups, labels=group_names, patch_artist=True)
    for i,(patch,g) in enumerate(zip(bp['boxes'],groups)):
        patch.set_facecolor(plt.cm.Set3(i/k))
        jitter = np.random.normal(0, .04, len(g))
        ax.scatter(np.full(len(g),i+1)+jitter, g, alpha=.4, s=15, color='black')
    ax.set_title(f'Kruskal-Wallis H={stat:.2f}, p={p_value:.4f}, η²={eta_sq:.3f} ({eta_label})')
    plt.show()
    
    result = {'statistic':stat, 'p_value':p_value, 'eta_squared':eta_sq}
    if p_value < 0.05:
        all_d = np.concatenate(groups)
        all_l = np.concatenate([[n]*len(g) for n,g in zip(group_names,groups)])
        df = pd.DataFrame({'value':all_d, 'group':all_l})
        dunn = sp.posthoc_dunn(df, val_col='value', group_col='group', p_adjust='bonferroni')
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(dunn, annot=True, fmt='.4f', cmap='RdYlGn_r', center=.05, ax=ax)
        ax.set_title('Dunn 사후 검정 (Bonferroni)'); plt.show()
        result['dunn_posthoc'] = dunn
    return result
```

**보고서 문장:** "Kruskal-Wallis 결과, 그룹 간 유의한 차이가 나타났다(H(df)=[값], p=[값], η²=[값]). Dunn 사후 검정에서 [GA]와 [GB] 간(p=[값])에서 유의한 차이 확인."

---

### 5.2 Friedman 검정

**이론:** 3개+ **반복측정** 조건 비교. 각 피험자 내에서 조건별 순위를 부여하고, 조건별 순위 합이 같은지 검정. 반복측정 ANOVA의 비모수 대안.

- **효과 크기:** Kendall's W = χ²/(n(k-1)), 범위 0~1

```python
def friedman_test(*conditions, condition_names=None):
    stat, p_value = friedmanchisquare(*conditions)
    k, n = len(conditions), len(conditions[0])
    W = stat / (n * (k-1))
    if condition_names is None: condition_names = [f'C{i+1}' for i in range(k)]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    for i in range(n):
        axes[0].plot(range(k), [c[i] for c in conditions], 'o-', alpha=.3, color='gray')
    axes[0].plot(range(k), [np.median(c) for c in conditions], 's-', color='red', lw=2, ms=10, label='중앙값')
    axes[0].set_xticks(range(k)); axes[0].set_xticklabels(condition_names); axes[0].legend()
    axes[0].set_title(f'Friedman χ²={stat:.2f}, p={p_value:.4f}')
    axes[1].boxplot(conditions, labels=condition_names, patch_artist=True)
    axes[1].set_title(f"Kendall's W={W:.3f}")
    plt.tight_layout(); plt.show()
    return {'statistic':stat, 'p_value':p_value, 'kendall_w':W}
```

---

## 6. 변곡점 분석

### 6.1 PELT (다중 변곡점)

**이론:** Pruned Exact Linear Time — 동적 프로그래밍으로 최적 분할을 찾되, 가지치기로 O(n) 복잡도 달성. Penalty로 변곡점 수 조절 (BIC 기반 자동 설정 권장).

```python
def detect_changepoints_pelt(data, model="rbf", penalty=None, name="Signal"):
    signal = np.array(data).reshape(-1, 1)
    if penalty is None: penalty = np.log(len(data)) * np.var(data)
    result = rpt.Pelt(model=model, min_size=2).fit(signal).predict(pen=penalty)
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 8))
    axes[0].plot(data, 'b-', lw=1.5, alpha=.8)
    prev = 0
    segs, labels = [], []
    for i, cp in enumerate(result):
        seg = data[prev:cp]; axes[0].hlines(np.median(seg), prev, cp, colors='red', lw=2.5)
        if cp != result[-1]: axes[0].axvline(cp, color='red', ls='--', alpha=.7)
        segs.append(seg); labels.append(f'구간{i+1}\n({prev}-{cp})'); prev = cp
    axes[0].set_title(f'{name}: PELT ({len(result)-1}개 변곡점)')
    axes[1].boxplot(segs, labels=labels, patch_artist=True)
    axes[1].set_title('구간별 분포')
    plt.tight_layout(); plt.show()
    return {'changepoints': result[:-1], 'n_segments': len(result)}
```

### 6.2 순위 기반 슬라이딩 윈도우

**이론:** Mann-Whitney U를 슬라이딩 윈도우로 반복 적용. 완전한 비모수적 접근. 이상치에 강건.

```python
def rank_based_changepoint(data, window=5, name="Signal"):
    n = len(data); p_values, positions = [], []
    for i in range(window, n-window):
        left, right = data[max(0,i-window):i], data[i:min(n,i+window)]
        if len(left)>=2 and len(right)>=2:
            _, p = mannwhitneyu(left, right, alternative='two-sided')
            p_values.append(p); positions.append(i)
    p_arr = np.array(p_values)
    cps = [positions[i] for i in range(1,len(p_arr)-1)
           if p_arr[i]<0.05 and p_arr[i]<p_arr[i-1] and p_arr[i]<p_arr[i+1]]
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
    axes[0].plot(data, 'b-', lw=1.5)
    for cp in cps: axes[0].axvline(cp, color='red', ls='--', alpha=.7)
    axes[0].set_title(f'{name}: 순위 기반 변곡점')
    axes[1].plot(positions, -np.log10(p_arr), 'g-', lw=1)
    axes[1].axhline(-np.log10(0.05), color='red', ls=':', label='α=0.05'); axes[1].legend()
    axes[1].set_ylabel('-log₁₀(p)')
    plt.tight_layout(); plt.show()
    return {'changepoints': cps}
```

---

## 7. 상관성 분석

### 7.1 Spearman ρ / Kendall τ

**이론:** 두 변수 간 **단조 관계** 측정. Pearson(선형만)과 달리 비선형 단조 관계도 포착.

- **Spearman:** 순위 간 Pearson 상관. n ≥ 20에서 권장
- **Kendall:** concordant/discordant 쌍 기반. 소표본(n < 20)에서 더 안정적. 동률에 강건

| ρ/τ 범위 | 해석 |
|----------|------|
| 0.00~0.19 | 무시할 수준 |
| 0.20~0.39 | 약한 상관 |
| 0.40~0.69 | 중간 상관 |
| 0.70~0.89 | 강한 상관 |
| 0.90~1.00 | 매우 강한 상관 |

```python
def spearman_correlation(x, y, x_name="X", y_name="Y"):
    rho, p_value = spearmanr(x, y)
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    axes[0].scatter(x, y, alpha=.6, edgecolors='white', s=80, c='steelblue')
    axes[0].set_xlabel(x_name); axes[0].set_ylabel(y_name)
    axes[0].set_title(f'Spearman ρ={rho:.3f}, p={p_value:.4f}')
    rx, ry = stats.rankdata(x), stats.rankdata(y)
    axes[1].scatter(rx, ry, alpha=.6, color='coral', s=80, edgecolors='white')
    z = np.polyfit(rx, ry, 1); axes[1].plot(np.sort(rx), np.poly1d(z)(np.sort(rx)), 'r--', lw=1.5)
    axes[1].set_xlabel(f'{x_name} (순위)'); axes[1].set_ylabel(f'{y_name} (순위)')
    axes[1].set_title('순위 변환')
    plt.tight_layout(); plt.show()
    return {'rho': rho, 'p_value': p_value}

def kendall_correlation(x, y):
    tau, p = kendalltau(x, y)
    return {'tau': tau, 'p_value': p}
```

### 7.2 다변량 상관 매트릭스

```python
def correlation_matrix_nonparametric(df, method='spearman'):
    cols = df.columns; corr = df.corr(method=method)
    p_mat = pd.DataFrame(np.ones((len(cols),len(cols))), columns=cols, index=cols)
    func = spearmanr if method=='spearman' else kendalltau
    for i,c1 in enumerate(cols):
        for j,c2 in enumerate(cols):
            if i!=j: _, p_mat.iloc[i,j] = func(df[c1], df[c2])
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)
    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[0], square=True)
    axes[0].set_title(f'{method.capitalize()} 상관')
    sns.heatmap(p_mat, mask=mask, annot=True, fmt='.3f', cmap='RdYlGn_r', center=.05, ax=axes[1], square=True)
    axes[1].set_title('p-value')
    plt.tight_layout(); plt.show()
    return {'correlation': corr, 'p_values': p_mat}
```

### 7.3 Distance Correlation (비선형 의존성)

**이론:** Pearson/Spearman이 탐지하지 못하는 **비선형·비단조 의존성**을 탐지. dCor=0 ⟺ 독립 (Pearson은 r=0이어도 의존 가능).

```python
from scipy.spatial.distance import pdist, squareform

def distance_correlation(x, y, n_perm=5000):
    def dcov(a, b):
        A = squareform(pdist(a.reshape(-1,1))); A -= A.mean(0); A -= A.mean(1,keepdims=True); A += A.mean()
        B = squareform(pdist(b.reshape(-1,1))); B -= B.mean(0); B -= B.mean(1,keepdims=True); B += B.mean()
        return np.sqrt(np.mean(A*B))
    
    dcov_xy = dcov(np.array(x), np.array(y))
    dcov_xx = dcov(np.array(x), np.array(x))
    dcov_yy = dcov(np.array(y), np.array(y))
    dcor = dcov_xy / np.sqrt(dcov_xx * dcov_yy) if dcov_xx*dcov_yy > 0 else 0
    
    perm_dcors = []
    y_arr = np.array(y)
    for _ in range(n_perm):
        np.random.shuffle(y_arr)
        d = dcov(np.array(x), y_arr) / np.sqrt(dcov_xx * dcov(y_arr,y_arr)) if dcov_xx > 0 else 0
        perm_dcors.append(d)
    p_value = np.mean(np.array(perm_dcors) >= dcor)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    axes[0].scatter(x, y, alpha=.6, s=60); axes[0].set_title(f'dCor={dcor:.3f}, p={p_value:.4f}')
    axes[1].hist(perm_dcors, bins=40, alpha=.7, color='lightgreen', edgecolor='white')
    axes[1].axvline(dcor, color='red', lw=2, label=f'관측 dCor={dcor:.3f}')
    axes[1].set_title('Permutation 분포'); axes[1].legend()
    plt.tight_layout(); plt.show()
    return {'dcor': dcor, 'p_value': p_value}
```

---

## 8. 정합성 검증

### 8.1 Bootstrap 신뢰구간

**이론:** 복원추출로 가상 표본을 반복 생성하여 통계량의 분포를 경험적으로 추정. 분포 가정 없이 어떤 통계량이든 CI 계산 가능.

```python
def bootstrap_ci(data, stat_func=np.median, n_boot=10000, ci=95, name="통계량"):
    np.random.seed(42)
    boots = [stat_func(np.random.choice(data, len(data), replace=True)) for _ in range(n_boot)]
    lo, hi = np.percentile(boots, [(100-ci)/2, 100-(100-ci)/2])
    obs = stat_func(data); se = np.std(boots)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.hist(boots, bins=50, density=True, alpha=.7, color='steelblue', edgecolor='white')
    ax.axvline(obs, color='red', lw=2, label=f'관측={obs:.3f}')
    ax.axvspan(lo, hi, alpha=.2, color='orange', label=f'{ci}% CI: [{lo:.3f},{hi:.3f}]')
    ax.set_title(f'{name} Bootstrap ({n_boot}회)'); ax.legend(); plt.show()
    return {'observed':obs, 'ci_lower':lo, 'ci_upper':hi, 'se':se}
```

### 8.2 Permutation Test

```python
def permutation_test(group1, group2, stat_func=np.median, n_perm=10000, name1="G1", name2="G2"):
    obs_diff = stat_func(group1) - stat_func(group2)
    combined = np.concatenate([group1, group2]); n1 = len(group1)
    np.random.seed(42)
    perms = []
    for _ in range(n_perm):
        np.random.shuffle(combined)
        perms.append(stat_func(combined[:n1]) - stat_func(combined[n1:]))
    p_value = np.mean(np.abs(perms) >= np.abs(obs_diff))
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.hist(perms, bins=50, density=True, alpha=.7, color='lightgreen', edgecolor='white')
    ax.axvline(obs_diff, color='red', lw=2, label=f'관측 차이={obs_diff:.3f}')
    ax.set_title(f'순열 검정: {name1} vs {name2} (p={p_value:.4f})'); ax.legend(); plt.show()
    return {'observed_diff': obs_diff, 'p_value': p_value}
```

### 8.3 Runs Test

**이론:** 중앙값 기준 이진화 후 "런" 수로 무작위성 검정. 런이 너무 적으면 추세/군집, 너무 많으면 교대 패턴.

```python
def runs_test_analysis(data, name="Data"):
    med = np.median(data); binary = (np.array(data)>=med).astype(int)
    runs = 1 + sum(1 for i in range(1,len(binary)) if binary[i]!=binary[i-1])
    n1, n0 = np.sum(binary==1), np.sum(binary==0); n = n1+n0
    exp = (2*n1*n0/n)+1; std = np.sqrt(2*n1*n0*(2*n1*n0-n)/(n**2*(n-1)))
    z = (runs-exp)/std if std>0 else 0; p = 2*(1-stats.norm.cdf(abs(z)))
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 8))
    axes[0].plot(data, 'b-o', ms=4); axes[0].axhline(med, color='red', ls='--', label=f'Mdn={med:.2f}')
    axes[0].fill_between(range(len(data)), med, data, where=np.array(data)>=med, alpha=.3, color='green')
    axes[0].fill_between(range(len(data)), med, data, where=np.array(data)<med, alpha=.3, color='red')
    axes[0].set_title(f'{name}: 런 검정'); axes[0].legend()
    axes[1].step(range(len(binary)), binary, 'k-', where='mid')
    axes[1].set_yticks([0,1]); axes[1].set_yticklabels(['미만','이상'])
    axes[1].set_title(f'런={runs}, 기대={exp:.1f}, Z={z:.2f}, p={p:.4f}')
    plt.tight_layout(); plt.show()
    return {'runs':runs, 'expected':exp, 'z':z, 'p_value':p}
```

---

## 9. 시나리오별 분석 워크플로

### A: A/B 테스트

| # | 단계 | 함수 |
|---|------|------|
| 1 | 그룹별 기술 통계 | `robust_descriptive()` |
| 2 | 정규성 검정 | `test_normality()` |
| 3 | 그룹 비교 | `mann_whitney_test()` |
| 4 | 중앙값 차이 신뢰구간 | `bootstrap_ci()` |

### B: 사전-사후 효과 측정

| # | 단계 | 함수 |
|---|------|------|
| 1 | 차이값 기술 통계 | `robust_descriptive()` |
| 2 | 사전-사후 비교 | `wilcoxon_paired_test()` |
| 3 | 차이 Bootstrap CI | `bootstrap_ci()` |

### C: 시계열 패턴 탐지

| # | 단계 | 함수 |
|---|------|------|
| 1 | 추세 검정 | `mann_kendall_test()` |
| 2 | 무작위성 검정 | `runs_test_analysis()` |
| 3 | 변곡점 탐지 | `pettitt_test()` / `detect_changepoints_pelt()` |
| 4 | 구간별 비교 | `mann_whitney_test()` |

### D: 다중 그룹 비교

| # | 단계 | 함수 |
|---|------|------|
| 1 | 전체 비교 | `kruskal_wallis_test()` |
| 2 | 사후 검정 (자동) | Dunn 내장 |

### E: 다변수 상관 탐색

| # | 단계 | 함수 |
|---|------|------|
| 1 | 상관 매트릭스 | `correlation_matrix_nonparametric()` |
| 2 | 비선형 의존성 | `distance_correlation()` |
| 3 | 상관계수 CI | `bootstrap_ci()` |
